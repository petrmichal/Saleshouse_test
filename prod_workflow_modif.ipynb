{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "### installs\n",
    "def run(jsn = 'missing'):\n",
    "  #!pip install google-cloud-automl\n",
    "  #!pip install google-cloud-storage\n",
    "  #!pip install opencv-python==3.3.0.9\n",
    "  #!pip install keras\n",
    "  #!pip install https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl\n",
    "  #!pip install tensorflow==1.12.0\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "### imports\n",
    "  \n",
    "  from google.cloud import automl_v1beta1 # Imports the Google Cloud client library\n",
    "  from google.cloud.automl_v1beta1.proto import service_pb2\n",
    "  #from google.datalab import storage as strg\n",
    "  from google.cloud import storage\n",
    "  from IPython.display import Image\n",
    "  from PIL import Image as Img\n",
    "  from imageai.Detection import ObjectDetection\n",
    "  from google.cloud.storage import Blob\n",
    "  import datetime\n",
    "  import google.cloud.bigquery as bq\n",
    "  import tensorflow as tf\n",
    "  import matplotlib.pyplot as plt\n",
    "  import math\n",
    "  import cv2\n",
    "  import numpy as np\n",
    "  import pandas as pd\n",
    "  import glob\n",
    "  import json\n",
    "  import io\n",
    "  from keras import backend as K\n",
    "  from pytz import timezone  \n",
    "  from  modules.logger import writelog\n",
    "  \n",
    "  import gc\n",
    "  import os\n",
    "  from skimage.color import rgb2lab, deltaE_cie76   ### alternative vertical shelves detection  \n",
    "  #import psutil  ## na monitorovani RAM a CPU\n",
    "  #import objgraph ## na monitorovani vyuziti pameti\n",
    "  import copy\n",
    "  import pickle  \n",
    "  import re  \n",
    "    \n",
    "# In[ ]:\n",
    "\n",
    "# env variables\n",
    "  output_bucket_name = 'saleshouse-test-output'\n",
    "  bq_dataset = 'db'\n",
    "\n",
    "# Instantiates a client\n",
    "  storage_client = storage.Client()\n",
    "\n",
    "  # The name for the new bucket\n",
    "  bucket_name = 'saleshousephotos'\n",
    "\n",
    "  bucket = storage_client.get_bucket(bucket_name)\n",
    "  blobs = bucket.list_blobs()\n",
    "\n",
    "  ######### toto tu nebude ##########\n",
    "  photos = []\n",
    "  for blob in blobs:\n",
    "    if 'photos' in blob.name:\n",
    "      photos.append(blob.name)\n",
    "  photos = photos[1:]\n",
    "  ###################################\n",
    "\n",
    "  ########## michal.mlaka ###########\n",
    "  # photos = URL k dane fotce ktera prijde\n",
    "  # pripluje photo_ID\n",
    "  if jsn == 'missing':\n",
    "    bucket_name = 'saleshouse-test-output'\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    jsn = Blob(\"newphoto.json\", bucket)\n",
    "    jsn = json.loads(jsn.download_as_string().decode('utf-8'))\n",
    "    modelpath = './'\n",
    "    outputpath = modelpath\n",
    " \n",
    "  photos = jsn['name']\n",
    "  bucket_name = jsn['bucket'] \n",
    "  bucket = storage_client.get_bucket(bucket_name)  \n",
    "  recognition_start = datetime.datetime.now(timezone('Europe/Berlin')).strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "  photo_load = pd.to_datetime(jsn['timeCreated']).tz_localize('UTC').tz_convert('Europe/Berlin').strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "  \n",
    "  if 'load_type' in jsn:\n",
    "      load_type = jsn['load_type']\n",
    "  else:\n",
    "      load_type = 'default'\n",
    " \n",
    "  modelpath = '/shelf-inspector/models/'\n",
    "  outputpath = '/shelf-inspector/tmp-outputs/'\n",
    "\n",
    " #jsn = {\"bucket\": \"saleshouse-test-pipeline\", \"contentType\": \"image/jpeg\", \"crc32c\": \"uH7RjA==\", \"etag\": \"CN/0h77T5d4CEAE=\", \"generation\": \"1542809022757471\", \"id\": \"saleshouse-test-pipeline/photos/15428090045761973780038.jpg/1542809022757471\", \"kind\": \"storage#object\", \"md5Hash\": \"S+F908zB4FNUDAoqL07QOA==\", \"mediaLink\": \"https://www.googleapis.com/download/storage/v1/b/saleshouse-test-pipeline/o/photos%2F15428090045761973780038.jpg?generation=1542809022757471&alt=media\", \"metageneration\": \"1\", \"name\": \"photos/15428090045761973780038.jpg\", \"selfLink\": \"https://www.googleapis.com/storage/v1/b/saleshouse-test-pipeline/o/photos%2F15428090045761973780038.jpg\", \"size\": \"2611476\", \"storageClass\": \"REGIONAL\", \"timeCreated\": \"2018-11-21T14:03:42.757Z\", \"timeStorageClassUpdated\": \"2018-11-21T14:03:42.757Z\", \"updated\": \"2018-11-21T14:03:42.757Z\"}\n",
    "  #bucket_name = jsn['bucket']\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "### functions neeeded\n",
    "\n",
    "  def blur_image(image, amount=5):\n",
    "      '''Blurs the image\n",
    "      Does not affect the original image'''\n",
    "      kernel = np.ones((amount, amount), np.float32) / (amount**2)\n",
    "      return cv2.filter2D(image, -1, kernel)\n",
    "\n",
    "  def colour_frame(img, frame, width=5, colour=[0, 0, 0]):\n",
    "      \"\"\"\"\"\"\n",
    "      x1, y1, x2, y2 = frame['box_points']\n",
    "      img[y1:y2, x1-width:x1+width] = colour\n",
    "      img[y1:y2, x2-width:x2+width] = colour\n",
    "      img[y1-width:y1+width, x1:x2] = colour\n",
    "      img[y2-width:y2+width, x1:x2] = colour\n",
    "      return img\n",
    "\n",
    "\n",
    "  def colour_map(shelf):\n",
    "      MAP = {0: [255, 0, 0], 1: [0, 0, 255], 2: [0, 255, 0], 3: [0, 255, 255], 4: [255, 255, 0], 5: [24, 49, 216], 6: [0, 209, 229], 7: [46, 60, 62]}\n",
    "      if shelf < 0:\n",
    "          return [0, 0, 0 ]\n",
    "      return MAP.get(shelf % len(MAP))\n",
    "\n",
    "\n",
    "  def compute_dominance_relations(detections):\n",
    "      \"\"\"\"\"\"\n",
    "      for i, frame in enumerate(detections):\n",
    "          x1, y1, x2, y2 = frame['box_points']\n",
    "          for j, frame_ in enumerate(detections):\n",
    "              x1_, y1_, x2_, y2_ = frame_['box_points']\n",
    "              if y2 < y1_:\n",
    "                  dominated_by = frame_.setdefault('dominated_by', set())\n",
    "                  dominated_by.add(i)\n",
    "\n",
    "\n",
    "  def compute_seed_shelves(detections, polish=1/4):\n",
    "      \"\"\"\"\"\"\n",
    "      frame_indeces = {i for i in range(len(detections))}    \n",
    "      shelf = -1\n",
    "      shelves = []\n",
    "      while frame_indeces:\n",
    "          shelf_frames = set()  \n",
    "          for i in frame_indeces:\n",
    "              frame = detections[i]\n",
    "  #            current_frame = colour_frame(main_obj, frame)\n",
    "  #            Image.fromarray(current_frame, 'RGB').show()\n",
    "              dominated_by = (detections[i] for i in frame.get('dominated_by', {}))\n",
    "              if all(dominating_frame.get('shelf', float('inf')) <= shelf \n",
    "                     for dominating_frame in dominated_by):\n",
    "                  x1, y1, x2, y2 = frame['box_points']\n",
    "                  upper_shelf_bottoms = [detections[i]['box_points'][3] for i in shelves[-1]] if shelf > -1 else []\n",
    "                  if sum(y2_ >= y2 - GAP for y2_ in upper_shelf_bottoms) > polish*len(upper_shelf_bottoms):\n",
    "  #                if all(y2_ >= y2 - GAP for y2_ in upper_shelf_bottoms):\n",
    "\n",
    "                      frame['shelf'] = shelf\n",
    "                      shelves[-1].add(i)\n",
    "                  else:\n",
    "                      frame['shelf'] = shelf + 1\n",
    "                      next_shelf = True\n",
    "                      shelf_frames.add(i)\n",
    "  #                main_obj = colour_frame(main_obj, frame, colour=colour_map(shelf))\n",
    "  #                img = Image.fromarray(main_obj, 'RGB')\n",
    "          frame_indeces -= shelf_frames\n",
    "          if shelf > -1:\n",
    "              frame_indeces -= shelves[-1]\n",
    "          shelves.append(shelf_frames)\n",
    "\n",
    "          shelf += 1 if next_shelf else 0\n",
    "      return detections\n",
    "\n",
    "\n",
    "  def detect_nonshelves(detections, main_obj):\n",
    "      \"\"\"\"\"\"\n",
    "      shelves = [set()  for _ in range(20)]\n",
    "      for iframe, frame in enumerate(detections):\n",
    "          shelves[frame['shelf']].add(iframe)\n",
    "\n",
    "      image_width = main_obj.shape[1]\n",
    "      to_remove = []\n",
    "      for ishelf, shelf in enumerate(shelves[:-1]):\n",
    "          lower_tops = [(detections[j]['box_points'][1], detections[j]['box_points'][2]) for j in shelves[ishelf+1]]\n",
    "          if not lower_tops:\n",
    "              continue\n",
    "          noshelf = []\n",
    "          for i in shelf:\n",
    "              frame = detections[i]\n",
    "              frame_bottom = frame['box_points'][3]\n",
    "              frame_top = frame['box_points'][1]\n",
    "              if sum(y1 < frame_bottom for y1, y2 in lower_tops) > 0*len(lower_tops)/2:\n",
    "  #                frame['shelf'] = -1\n",
    "                  noshelf.append(frame)\n",
    "  #        Image.fromarray(colour_shelves(noshelf, main_obj.copy()), 'RGB').show()\n",
    "          safety_condition = noshelf and (min(f['box_points'][0] for f in noshelf) > image_width*1/2\n",
    "                          or max(f['box_points'][2] for f in noshelf) < image_width*1/2)\n",
    "          if safety_condition:\n",
    "              for f in noshelf:\n",
    "                  f['shelf'] = -1\n",
    "              to_remove.extend(noshelf)\n",
    "      for frame in detections:\n",
    "          if to_remove:\n",
    "              if frame['box_points'][2] > image_width/2:\n",
    "                  if frame['box_points'][2] >= min(f['box_points'][0] for f in to_remove) > image_width/2:\n",
    "                      frame['shelf'] = -1\n",
    "              elif frame['box_points'][2] < image_width/2:\n",
    "                  if frame['box_points'][0] <= max(f['box_points'][2] for f in to_remove) < image_width/2:\n",
    "                      frame['shelf'] = -1\n",
    "      return detections\n",
    "\n",
    "\n",
    "  def detect_shelves(detections, polish=1/4):\n",
    "      \"\"\"Detekuje regaly na zaklade detekci lahvi. Vadi FALSE POSITIVES.\n",
    "      V prvni iteraci je ale treba ponechat vysi FALSE POSITVES, abychom meli nizsi FALSE NEGATIVES,\n",
    "      nebot FALSE NEGATIVES zase budou vadit nasledujicimu `correct` kroku.\n",
    "      V pripade ze se v dalsich iteracich snizi FALSE POSITIVES, bude mozna treba zvysovat parametr `polish` smerem k 0.99.\n",
    "      Parametr polish sleva \"falesne regaly\" do jednoho.\n",
    "      Kazdy dict v `detections` dostane novy atribut `shelf` \n",
    "      \"\"\"\n",
    "      compute_dominance_relations(detections)   \n",
    "      detections = compute_seed_shelves(detections, polish)\n",
    "      return detections\n",
    "\n",
    "  def correct_shelves(detections, main_obj):\n",
    "      \"\"\"Odstrani lahve stojici v sousednich regalech. Vadi FALSE NEGATIVES. \n",
    "      Prvky v `detections` k odstraneni dostanou `shelf=-1`\n",
    "      Input\n",
    "      ----\n",
    "      detections : list of dicts\n",
    "          detections with `shelf` attribute from `detect` method\n",
    "      main_obj : array \n",
    "          image from the bottle detector\n",
    "      \"\"\"\n",
    "      detections = detect_nonshelves(detections, main_obj)\n",
    "      return detections\n",
    "\n",
    "  def colour_shelves(detections, main_obj):\n",
    "      \"\"\"\"\"\"\n",
    "      for frame in detections:\n",
    "          if 'shelf' in frame:\n",
    "              main_obj = colour_frame(main_obj, frame, colour=colour_map(frame['shelf']))\n",
    "      return main_obj\n",
    "\n",
    "  def colour_shelves_prediction(detections, main_obj):\n",
    "      \"\"\"\"\"\"\n",
    "      for i, frame in enumerate(detections):\n",
    "        if 'class' in frame:\n",
    "            if frame['class'] == 'rb':\n",
    "              main_obj = colour_frame(main_obj, frame, colour=colour_map(4))\n",
    "            elif frame['class'] == 'rbsf':\n",
    "              main_obj = colour_frame(main_obj, frame, colour=colour_map(6))\n",
    "            elif frame['class'] == 'rbred':\n",
    "              main_obj = colour_frame(main_obj, frame, colour=colour_map(2))\n",
    "            elif frame['class'] == 'rbblue':\n",
    "              main_obj = colour_frame(main_obj, frame, colour=colour_map(1))\t\t\t \n",
    "            elif frame['class'] == 'bigshock':\n",
    "              main_obj = colour_frame(main_obj, frame, colour=colour_map(0))\t\t\t \n",
    "            elif frame['class'] == 'rbturned':\n",
    "              main_obj = colour_frame(main_obj, frame, colour=colour_map(4))\t            #else: \n",
    "              #main_obj = colour_frame(main_obj, frame, colour=colour_map(0))\n",
    "      return main_obj\n",
    "\n",
    "\n",
    "  def rotateImage(image, angle):\n",
    "    img = Img.fromarray(image, 'RGB')\n",
    "    img = img.rotate(angle, expand=True)\n",
    "    img = np.array(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "  def detectVertShelf(image, color_threshold=10, minLineLength=300):\n",
    "    \"\"\"Detekuje svisle hrany (regaly) na zaklade metriky na RGB.\n",
    "        Porovna \"vzdalenost\" barvy regalu od barev v regalu, vzdalenym pixelum da cernou barvu,\n",
    "        pak se v takovem obrazku detekuji hrany  \n",
    "\n",
    "      image ... vstupni obrazek, jako numpy array\n",
    "      color_threshold ... threshold vzdalenosti barvy od barvy regalu\n",
    "      minLineLength ... minimalni deka hrany pro detekci\n",
    "      \n",
    "      vraci puvodni obrazek a detekovane lines (viz cv2.HoughLinesP doc)\n",
    "    \"\"\"\n",
    "\n",
    "    lab = rgb2lab(image)\n",
    "    regal = [225, 220, 180]   ### approx. color of shelf (globus sediva)\n",
    "    \n",
    "    regal_3d = np.uint8(np.asarray([[regal]]))\n",
    "    dE_regal= deltaE_cie76(rgb2lab(regal_3d), lab)\n",
    "    \n",
    "    image_res = image.copy()\n",
    "    image_res[dE_regal >= color_threshold] = [0,0,0]   ## far away from shelf color -> black\n",
    "    gray = cv2.cvtColor(image_res, cv2.COLOR_BGR2GRAY)\n",
    "    #minLineLength=100\n",
    "    lines = cv2.HoughLinesP(image=gray,rho=1,theta=np.pi, threshold=20,lines=np.array([]), minLineLength=minLineLength, maxLineGap=3)\n",
    "    \n",
    "    return image, lines\n",
    "  \n",
    "  \n",
    "  def shiftDetections(detec, shift):\n",
    "    ### shift the boxes about the difference of img_chopped and unchopped img\n",
    "   \n",
    "    for i in range(len(detec)):\n",
    "      #print(i)\n",
    "      detec[i]['box_points'][0] = shift + detec[i]['box_points'][0]\n",
    "      detec[i]['box_points'][2] = shift + detec[i]['box_points'][2]\n",
    "    \n",
    "    return detec  \n",
    "\n",
    "\n",
    "  def planogram_check(obj, planograms, row, column, store_location, date, start_pix=None, end_pix=None):\n",
    "    \"\"\"\n",
    "    The function takes as input the information from both photo product\n",
    "    recognition reality and the target planogram. It outputs a DataFrame\n",
    "    containing aggregated information about their similarity.\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj : Dictionary\n",
    "        Contains information about pixel allocations to different products\n",
    "        in a given row and column. Example input format:\n",
    "            obj = {\"label\": [\"rblue\", ...], \"location\" : [[(0,0), (121, 134),...],[...],...]}\n",
    "    planograms : Dictionary\n",
    "        Contains parsed planograms. It is an output from read_all_planograms().\n",
    "    row : Integer\n",
    "        Row location of the obj in the whole shelf.\n",
    "    column : Integer\n",
    "        Row location of the obj in the whole shelf.\n",
    "    store_location : String\n",
    "        Location of the store from the target planogram file name. Example\n",
    "        input: \"Zlicin\"\n",
    "    start_pix : Integer\n",
    "        Starting (leftmost) pixel of the given cell.\n",
    "    end_pix : Integer\n",
    "        Last (rightmost) pixel of the given cell.\n",
    "    Returns\n",
    "    -------\n",
    "    plan_check_df : DataFrame\n",
    "        DataFrame containing aggregated information about similarity between\n",
    "        planogram and photo reality\n",
    "    \"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# load the target planogram part\n",
    "# =============================================================================\n",
    "    # find the correct index in the planograms dictionary\n",
    "    plan_dates = list(map(lambda x:x.split(\"_\")[0],planograms[\"plan_name\"]))\n",
    "    plan_stores = list(map(lambda x:x.split(\"_\")[2],planograms[\"plan_name\"]))\n",
    "    plan_names_df = pd.DataFrame({\"plan_dates\" : plan_dates, \"plan_stores\": plan_stores})\n",
    "    target_ind = plan_names_df.index[(plan_names_df[\"plan_dates\"] == date) & (plan_names_df[\"plan_stores\"] == store_location)].tolist()[0]\n",
    "    # get the target planogram into df\n",
    "    plan = planograms[\"plan_df\"][target_ind]\n",
    "    # zoom into the target planogram part\n",
    "    plan = plan.loc[(plan['row'] == row) & (plan['column'] == column)]\n",
    "\n",
    "# =============================================================================\n",
    "# transform photo reality into checking matrix\n",
    "# =============================================================================\n",
    "    labels = obj[\"label\"]\n",
    "    num_labels = len(labels)\n",
    "    # starting and ending pixels for measuring row size\n",
    "    pixels = [item for sublist in obj[\"location\"] for item in sublist]\n",
    "    pixels = np.array(sum(pixels, ()))\n",
    "    start_pix = min(pixels[np.nonzero(pixels)]) if start_pix is None else start_pix # minimal nonzero\n",
    "    end_pix = max(pixels) if end_pix is None else end_pix\n",
    "    row_size = end_pix - start_pix + 1\n",
    "\n",
    "    # checking matrix dimensions: 0 - label, 1 - pixels\n",
    "    real_check_mat = np.full((num_labels, row_size), 0 ,dtype=float)\n",
    "\n",
    "    # fill the checking matrix in a loop over products\n",
    "    for i in range(num_labels):\n",
    "        # pixels containing the target label product\n",
    "        label_pixels = obj[\"location\"][i][1:] # remove (0,0)\n",
    "        label_pixels = np.array(sum(label_pixels, ())) - start_pix\n",
    "        # write into the table, loop trough starting pixel starting points\n",
    "        for p in range(int(len(label_pixels)/2)):\n",
    "            real_check_mat[i, label_pixels[2 * p] : label_pixels[2 * p + 1] + 1] = 1\n",
    "\n",
    "# =============================================================================\n",
    "# transform planogram into checking matrix\n",
    "# =============================================================================\n",
    "    # checking matrix dimensions: 0 - label, 1 - pixels\n",
    "    plan_check_mat = np.full((num_labels, row_size), 0 ,dtype=float)\n",
    "\n",
    "    # fill the checking matrix in a loop over products\n",
    "    for i in range(num_labels):\n",
    "        # filter down to the given label\n",
    "        plan_label = plan.loc[plan['label'] == labels[i]]\n",
    "        plan_label = plan_label.reset_index().drop(columns=\"index\")\n",
    "        # loop trough individual blocks and write into the table\n",
    "        for b in range(plan_label.shape[0]):\n",
    "            # translate cell share start and end points to pixels\n",
    "            cell_share_start_pix = int(np.ceil(plan_label.loc[b, \"cell_share_start\"] * row_size))\n",
    "            cell_share_end_pix = int(np.ceil(plan_label.loc[b, \"cell_share_end\"] * row_size))\n",
    "            plan_check_mat[i, cell_share_start_pix : cell_share_end_pix] = 1\n",
    "\n",
    "# =============================================================================\n",
    "# Checking similarity between photo reality and planogram\n",
    "# =============================================================================\n",
    "    # share of cell where the products was planned but is missing\n",
    "    missing_share = np.mean((real_check_mat < plan_check_mat)*1, axis=1)\n",
    "\n",
    "    # share of cell where the product was not planned but is there\n",
    "    extra_share = np.mean((real_check_mat > plan_check_mat)*1, axis=1)\n",
    "\n",
    "    # originally planned\n",
    "    planogram_share = np.mean((plan_check_mat)*1, axis=1)\n",
    "\n",
    "    # combining into the resulting output\n",
    "    plan_check_df = pd.DataFrame({\"label\": labels,\n",
    "                                  \"planogram_share\": planogram_share,\n",
    "                                  \"missing_share\": missing_share,\n",
    "                                  \"extra_share\": extra_share})\n",
    "\n",
    "    return plan_check_df\n",
    "\n",
    "  def parse_store_location(store_string):\n",
    "    \"\"\"\n",
    "    The function converts store string from the photo loading web application\n",
    "    and transforms it into the correct store_location input for the\n",
    "    planogram_check() function.\n",
    "    Parameters\n",
    "    ----------\n",
    "    store_string : String\n",
    "        String from web application containing store identification code\n",
    "        in the round brackets. Example format:\n",
    "            \"Poděbradská 293 Pardubice Pardubice VII 53009 (S2318CZ)\"\n",
    "    Returns\n",
    "    -------\n",
    "    store_location : String\n",
    "        String contatining unique store location name. It serves as an output\n",
    "        to the planogram_check() function.\n",
    "    \"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# Create DataFrame of store codes and names\n",
    "# =============================================================================\n",
    "    store_names = [\"Chomutov\", \"Ceske Budejovice\", \"Brno\",\n",
    "                   \"Cerny Most\", \"Plzen\", \"Cakovice\",\n",
    "                   \"Karlovy Vary\", \"Ostrava\", \"Pardubice\",\n",
    "                   \"Olomouc\", \"Zlicin\", \"Liberec\",\n",
    "                   \"Opava\", \"Havirov\", \"Usti\"]\n",
    "\n",
    "    store_codes = [\"S2325CZ\", \"S2321CZ\", \"S2320CZ\",\n",
    "                   \"S2323CZ\", \"S2324CZ\", \"S2315CZ\",\n",
    "                   \"S2316CZ\", \"S2329CZ\", \"S2318CZ\",\n",
    "                   \"S2322CZ\", \"S2753CZ\", \"S2317CZ\",\n",
    "                   \"S2328CZ\", \"S2995CZ\", \"S2319CZ\"]\n",
    "    # create the code df\n",
    "    df_store_codes = pd.DataFrame({\"store_codes\": store_codes,\n",
    "                                   \"store_names\": store_names})\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Extract the code and transform it into the store names\n",
    "# =============================================================================\n",
    "\n",
    "    # extract store identification code\n",
    "    id_code = re.search('\\((.*)\\)', store_string).group(1)\n",
    "\n",
    "    # find the appropriate name for the code in the store_codes df\n",
    "    store_location = df_store_codes.loc[df_store_codes[\"store_codes\"] == id_code, \"store_names\"].values[0]\n",
    "\n",
    "    return store_location\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "### vyhodit???\n",
    " # from __future__ import absolute_import\n",
    " # from __future__ import division\n",
    " # from __future__ import print_function\n",
    "\n",
    "  ### functions for local model\n",
    "  def load_graph(model_file):\n",
    "    graph = tf.Graph()\n",
    "    graph_def = tf.GraphDef()\n",
    "\n",
    "    with open(model_file, \"rb\") as f:\n",
    "      graph_def.ParseFromString(f.read())\n",
    "    with graph.as_default():\n",
    "      tf.import_graph_def(graph_def)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "  def read_tensor_from_image_file(file_name,\n",
    "                                  input_height=299,\n",
    "                                  input_width=299,\n",
    "                                  input_mean=0,\n",
    "                                  input_std=255):\n",
    "    input_name = \"file_reader\"\n",
    "    output_name = \"normalized\"\n",
    "    file_reader = tf.read_file(file_name, input_name)\n",
    "    if file_name.endswith(\".png\"):\n",
    "      image_reader = tf.image.decode_png(\n",
    "          file_reader, channels=3, name=\"png_reader\")\n",
    "    elif file_name.endswith(\".gif\"):\n",
    "      image_reader = tf.squeeze(\n",
    "          tf.image.decode_gif(file_reader, name=\"gif_reader\"))\n",
    "    elif file_name.endswith(\".bmp\"):\n",
    "      image_reader = tf.image.decode_bmp(file_reader, name=\"bmp_reader\")\n",
    "    else:\n",
    "      image_reader = tf.image.decode_jpeg(\n",
    "          file_reader, channels=3, name=\"jpeg_reader\")\n",
    "    float_caster = tf.cast(image_reader, tf.float32)\n",
    "    dims_expander = tf.expand_dims(float_caster, 0)\n",
    "    resized = tf.image.resize_bilinear(dims_expander, [input_height, input_width])\n",
    "    normalized = tf.divide(tf.subtract(resized, [input_mean]), [input_std])\n",
    "    sess = tf.Session()\n",
    "    result = sess.run(normalized)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "  def load_labels(label_file):\n",
    "    label = []\n",
    "    proto_as_ascii_lines = tf.gfile.GFile(label_file).readlines()\n",
    "    for l in proto_as_ascii_lines:\n",
    "      label.append(l.rstrip())\n",
    "    return label\n",
    "\n",
    "###. dummy comment\n",
    "  def local_recognizer(file_name, graph):\n",
    "    input_height = 299\n",
    "    input_width = 299\n",
    "    input_mean = 0\n",
    "    input_std = 255\n",
    "    input_layer = \"Mul\"\n",
    "    output_layer = \"final_result\"\n",
    "\n",
    "    #graph = load_graph(model_file)\n",
    "    t = read_tensor_from_image_file(file_name,\n",
    "          input_height=input_height,\n",
    "          input_width=input_width,\n",
    "          input_mean=input_mean,\n",
    "          input_std=input_std)\n",
    "\n",
    "    input_name = \"import/\" + input_layer\n",
    "    output_name = \"import/\" + output_layer\n",
    "    input_operation = graph.get_operation_by_name(input_name)\n",
    "    output_operation = graph.get_operation_by_name(output_name)\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "      results = sess.run(output_operation.outputs[0], {\n",
    "          input_operation.outputs[0]: t\n",
    "      })\n",
    "    results = np.squeeze(results)\n",
    "\n",
    "    top_k = results.argsort()[-5:][::-1]\n",
    "    labels = load_labels(label_file)\n",
    "    l = top_k[0]\n",
    "\n",
    "    return labels[l], results[l]\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "  print('OK fn')   \n",
    "### detection of objects for chopping the photo\n",
    "\n",
    "  #for i in range(len(photos)):\n",
    "  for i in range(1,2):\n",
    "    tmp = bucket.blob(photos).download_as_string()\n",
    "    img = Img.open(io.BytesIO(tmp))\n",
    "\n",
    "    info = img._getexif()\n",
    "    for tag, value in info.items():\n",
    "        key = TAGS.get(tag)\n",
    "        if key == 'Orientation':\n",
    "            print(key + ': ' + str(value))\n",
    "            orientation=value \n",
    "    \n",
    "    img = np.array(img)\n",
    "\n",
    "    which_rot = 0  ## indicates if image was rotated\n",
    "    \n",
    "    if orientation == 3:         ### otoceni kvuli spatnemu nacteni \n",
    "      img = rotateImage(img, 180)\n",
    "      which_rot = 180\n",
    "    elif orientation == 6:  \n",
    "      img = rotateImage(img, 90)\n",
    "      which_rot = 90\n",
    "    elif orientation == 8:\n",
    "      img = rotateImage(img, -90)\n",
    "      which_rot = -90\n",
    "\n",
    "    img = np.array(img)\n",
    "\n",
    "    #gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    #edges = cv2.Canny(gray, 250, 250)\n",
    "    #image = blur_image(edges, amount = 7)\n",
    "\n",
    "    \n",
    "    \n",
    "    image, lines = detectVertShelf(img, 30, img.shape[0]/7)\n",
    "   \n",
    "    #lines = cv2.HoughLinesP(image, rho = 1, theta = math.pi, threshold = 20, minLineLength = img.shape[0]/1.5, maxLineGap = 0)\n",
    "    #if lines is None:\n",
    "    #    lines = cv2.HoughLinesP(image, rho = 1, theta = math.pi, threshold = 50, minLineLength = img.shape[0]/2.5, maxLineGap = 3)\n",
    "\n",
    "    img2 = img.copy()\n",
    "    if lines is None:\n",
    "      img_chopped = img.copy()\n",
    "    else:\n",
    "      lines = sorted(lines,key=lambda x: x[0,0])\n",
    "      dominance = -1000\n",
    "      right_chop = img.shape[1]\n",
    "      left_chop = 0\n",
    "\n",
    "      for line2 in range(len(lines)):\n",
    "          line = lines[line2]\n",
    "          if line[0,0] - dominance > 0.2*img.shape[1]:\n",
    "              if (line[0,0] < 0.2*img.shape[1] or line[0,0] > 0.8*img.shape[1]):\n",
    "                  if (line[0,0] < 0.2*img.shape[1]):\n",
    "                    left_chop = line[0,0]\n",
    "                  if (line[0,0] > 0.8*img.shape[1]):\n",
    "                    right_chop = line[0,0]\n",
    "                  pt1 = (line[0,0],line[0,1])\n",
    "                  pt2 = (line[0,2],line[0,3])\n",
    "                  #cv2.line(img, pt1, pt2, (0,0,255), 3)\n",
    "                  dominance = line[0,0]\n",
    "          pt1 = (line[0,0],line[0,1])\n",
    "          pt2 = (line[0,2],line[0,3])\n",
    "          cv2.line(img2, pt1, pt2, (0,0,255), 3)\n",
    "\n",
    "      img_chopped = img[:,left_chop:right_chop,:]\n",
    "    PERC = 30\n",
    "\n",
    "    print('image ok')\n",
    "    writelog('Image successfully prepared for recognition')\n",
    "\n",
    "    detector = ObjectDetection()\n",
    "    detector.setModelTypeAsRetinaNet()\n",
    "\n",
    "    model = \"{}temp_model.h5\".format(modelpath)\n",
    "    detector.setModelPath(model)\n",
    "\n",
    "    writelog('1.')\n",
    "\n",
    "\n",
    "    writelog('1.')\n",
    "    detector.loadModel()\n",
    "\n",
    "    print('model OK')\n",
    "    writelog('Model successfully prepared.')\n",
    "\n",
    "    custom_objects = detector.CustomObjects(person=False, car=False, bottle = True)\n",
    "    main_obj, detections, extracted_obj = detector.detectCustomObjectsFromImage(input_image= img_chopped, \n",
    "                                                        input_type=\"array\",\n",
    "                                                        output_type = 'array', \n",
    "                                                        output_image_path= \"{}im.png\".format(outputpath), \n",
    "                                                        custom_objects=custom_objects,\n",
    "                                                        extract_detected_objects=True,\n",
    "                                                        minimum_percentage_probability=PERC)\n",
    "\n",
    "    \n",
    "      \n",
    "    limit_det = 3   ### minimum bńumber of detections, if less then turn the image\n",
    "    if len(detections)<=limit_det:\n",
    "      writelog('Otoceni')\n",
    "      img_chopped_rot = rotateImage(img_chopped, 90)\n",
    "      main_obj_2, detections_2, extracted_obj_2 = detector.detectCustomObjectsFromImage(input_image= img_chopped_rot, \n",
    "                                                        input_type=\"array\",\n",
    "                                                        output_type = 'array', \n",
    "                                                        output_image_path= \"./im.png\", \n",
    "                                                        custom_objects=custom_objects,\n",
    "                                                        extract_detected_objects=True,\n",
    "                                                        minimum_percentage_probability=PERC)\n",
    "       \n",
    "      if len(detections_2) > limit_det:\n",
    "        main_obj = main_obj_2\n",
    "        detections = detections_2\n",
    "        extracted_obj = extracted_obj_2\n",
    "        img_chopped = img_chopped_rot\n",
    "        which_rot = 90\n",
    "      else:  \n",
    "        img_chopped_rot = rotateImage(img_chopped, -90)\n",
    "        main_obj_3, detections_3, extracted_obj_3 = detector.detectCustomObjectsFromImage(input_image= img_chopped_rot, \n",
    "                                                        input_type=\"array\",\n",
    "                                                        output_type = 'array', \n",
    "                                                        output_image_path= \"./im.png\", \n",
    "                                                        custom_objects=custom_objects,\n",
    "                                                        extract_detected_objects=True,\n",
    "                                                        minimum_percentage_probability=PERC)\n",
    "        if len(detections_3) > limit_det:\n",
    "            main_obj = main_obj_3\n",
    "            detections = detections_3\n",
    "            extracted_obj = extracted_obj_3\n",
    "            img_chopped = img_chopped_rot\n",
    "            which_rot = -90\n",
    "            \n",
    "       \n",
    "    print(len(detections), 'detekci z prvni detekce')\n",
    "    \n",
    "    \n",
    "    detections_all = detections.copy()\n",
    "    extracted_all = extracted_obj.copy()\n",
    "    img_chopped_black = img_chopped.copy()\n",
    "    \n",
    "    \n",
    "        \n",
    "    for i in range(len(detections)):\n",
    "      img_chopped_black[detections[i]['box_points'][1]:detections[i]['box_points'][3],detections[i]['box_points'][0]:detections[i]['box_points'][2],:] = 0\n",
    "    PERC = 5\n",
    "\n",
    "    writelog('Second detection')\n",
    "\n",
    "    detector = ObjectDetection()\n",
    "    detector.setModelTypeAsRetinaNet() \n",
    "    detector.setModelPath(model)\n",
    "\n",
    "    detector.loadModel()\n",
    "\n",
    "    custom_objects = detector.CustomObjects(person=False, car=False, bottle = True)\n",
    "    main_obj, detections, extracted_obj = detector.detectCustomObjectsFromImage(input_image= img_chopped_black, \n",
    "                                                        input_type=\"array\",\n",
    "                                                        output_type = 'array', \n",
    "                                                        output_image_path=\"{}im.png\".format(outputpath), \n",
    "                                                        custom_objects=custom_objects,\n",
    "                                                        extract_detected_objects=True,\n",
    "                                                        minimum_percentage_probability=PERC)\n",
    "    for i in detections:        \n",
    "      detections_all.append(i)\n",
    "    for i in extracted_obj:   \n",
    "      extracted_all.append(i)\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    shelf_tuple = [(l['box_points'][0], l['box_points'][2]) for l in detections_all]\n",
    "    shelf_tuple_y = [(l['box_points'][1], l['box_points'][3]) for l in detections_all]\n",
    "    shelf_tuple_wo = shelf_tuple.copy()\n",
    "    shelf_tuple.append((0,0))\n",
    "    shelf_tuple.append((img_chopped.shape[1], img_chopped.shape[1]))\n",
    "    sorted_by_lower_bound = sorted(shelf_tuple, key=lambda tup: tup[0])\n",
    "    merged = []\n",
    "\n",
    "    for higher in sorted_by_lower_bound:\n",
    "          if not merged:\n",
    "              merged.append(higher)\n",
    "          else:\n",
    "              lower = merged[-1]\n",
    "              if higher[0] <= lower[1]:\n",
    "                  upper_bound = max(lower[1], higher[1])\n",
    "                  merged[-1] = (lower[0], upper_bound)  # replace by merged interval\n",
    "              else:\n",
    "                  merged.append(higher)\n",
    "    remove = []\n",
    "    for i in range(len(merged)-1):\n",
    "      if merged[i+1][0]-merged[i][1] > img_chopped.shape[1]*0.05:\n",
    "        remove.append((merged[i][1], merged[i+1][0]))\n",
    "\n",
    "    img_chopped_fin = img_chopped.copy()\n",
    "\n",
    "### zatim zakomentavane, TD: zjistit, o kolik se tu orizne -> pricist k left_chop\n",
    "#     down_grade = 0\n",
    "#     for i in remove:\n",
    "#       #img_chopped_fin = img_chopped_fin[:,0:i[0] - down_grade + 1,:] + img_chopped_fin[:,i[1]- down_grade - 1:,:]\n",
    "#       img_chopped_fin = img_chopped_fin[:, np.r_[0:i[0] - down_grade + 1,i[1]- down_grade - 1: img_chopped_fin.shape[1]], :]\n",
    "#       down_grade += i[1] - i[0]\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "### sizes of boxes\n",
    "\n",
    "  sizes_x = [l[1] - l[0] for l in shelf_tuple_wo]\n",
    "  sizes_y = [l[1] - l[0] for l in shelf_tuple_y]\n",
    "  sizes_x_arr = np.array(sizes_x)\n",
    "  sizes_y_arr = np.array(sizes_y)\n",
    "  #plt.hist(sizes_x_arr, bins = 20)\n",
    "\n",
    "  if np.mean(sizes_x_arr) - 2.5*np.std(sizes_x_arr) < np.percentile(sizes_x_arr, 5):\n",
    "    cut_lower_x = np.mean(sizes_x_arr) - 2.5*np.std(sizes_x_arr) \n",
    "  else:\n",
    "    cut_lower_x = np.percentile(sizes_x_arr, 5)\n",
    "\n",
    "  if np.mean(sizes_x_arr) + 2.5*np.std(sizes_x_arr) > np.percentile(sizes_x_arr, 95):\n",
    "    cut_upper_x = np.mean(sizes_x_arr) + 2.5*np.std(sizes_x_arr)\n",
    "  else:\n",
    "    cut_upper_x = np.percentile(sizes_x_arr, 95)\n",
    "\n",
    "  if np.mean(sizes_y_arr) - 2.5*np.std(sizes_y_arr) < np.percentile(sizes_y_arr, 5):\n",
    "    cut_lower_y = np.mean(sizes_y_arr) - 2.5*np.std(sizes_y_arr) \n",
    "  else:\n",
    "    cut_lower_y = np.percentile(sizes_y_arr, 5)\n",
    "\n",
    "  if np.mean(sizes_y_arr) + 2.5*np.std(sizes_y_arr) > np.percentile(sizes_y_arr, 95):\n",
    "    cut_upper_y = np.mean(sizes_y_arr) + 2.5*np.std(sizes_y_arr)\n",
    "  else:\n",
    "    cut_upper_y = np.percentile(sizes_y_arr, 95)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "### shelf detector (from detections + domination, without poles)\n",
    "  GAP = 15\n",
    "  detections = detect_shelves(detections_all) # zaradi do regalu\n",
    "  detections = correct_shelves(detections, img_chopped_fin)  # oznaci ty co nejsou v zadnem regalu\n",
    "  \n",
    "  shelves = list()   ### filter out shelves with <= .. detected objects\n",
    "  for i in range(len(detections)):  \n",
    "    shelves.append(detections[i]['shelf'])  ## separate shelf numbers\n",
    "  x=np.array(shelves)\n",
    "  unique, counts = np.unique(x, return_counts=True)   ### frequency\n",
    "  \n",
    "  if unique[0]== -1:\n",
    "    counts2= counts[1:len(counts)]   ### eliminate -1 shelf\n",
    "    unique2= unique[1:len(unique)]\n",
    "  else:\n",
    "    counts2=counts\n",
    "    unique2=unique\n",
    "\n",
    "  shelves_ok = list()\n",
    "  for i in range(len(counts2)):\n",
    "    if counts2[i]>=4:   ### choose shelves\n",
    "      shelves_ok.append(unique2[i])\n",
    "  \n",
    "  detections_ok = list()   ### filter detections in ok shelves\n",
    "  extracted_all_ok = list()\n",
    "  for i in range(len(detections)):\n",
    "    if detections[i]['shelf'] in shelves_ok:\n",
    "      detections_ok.append(detections[i])\n",
    "      extracted_all_ok.append(extracted_all[i])\n",
    "  \n",
    "  \n",
    "  detections_ok_shift = shiftDetections(copy.deepcopy(detections_ok), left_chop)\n",
    "  main_obj = colour_shelves(detections_ok_shift, img_chopped_fin.copy())\n",
    "  #img_chopped_fin\n",
    "  #Img.fromarray(main_obj, 'RGB')\n",
    "  #Img.fromarray(img, 'RGB')\n",
    "  \n",
    " \n",
    "### reindexing of shelves, \"0:number\"\n",
    "\n",
    "  unique_po, counts_po = np.unique(np.array(shelves_ok), return_counts=True)\n",
    "  seq=list()\n",
    "  for i in range(len(unique_po)):\n",
    "    seq.append(i)\n",
    "  \n",
    "  d = {unique_po[i]: seq[i] for i in range(len(seq))}\n",
    "  \n",
    "  detections_ok_reshelved = copy.deepcopy(detections_ok)\n",
    "  for i in range(len(detections_ok_reshelved)):\n",
    "    detections_ok_reshelved[i]['shelf'] = d.get(detections_ok[i]['shelf'])\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "### filter empty vertical stripes of photo + filter out weird detected boxes\n",
    "\n",
    "  shelf_tuple = list()\n",
    "  detections_wo = list()\n",
    "  extractions_wo = list()\n",
    "  shlvs = list()\n",
    "\n",
    "  for idx, l in enumerate(detections_ok_reshelved):\n",
    "              if l['shelf'] == -1:\n",
    "                  shelf_tuple.append( (l['box_points'][0], l['box_points'][2]) )\n",
    "              else:\n",
    "                if ((l['box_points'][2] - l['box_points'][0]) > cut_lower_x) or ((l['box_points'][2] - l['box_points'][0]) < cut_upper_x) or ((l['box_points'][3] - l['box_points'][1]) > cut_lower_y) or ((l['box_points'][3] - l['box_points'][1]) < cut_upper_y) :\n",
    "                  detections_wo.append(l)\n",
    "                  extractions_wo.append(extracted_all_ok[idx])\n",
    "                  shlvs.append(l['shelf'])\n",
    "  shelf_tuple.append((0,0))\n",
    "  shelf_tuple.append((img_chopped_fin.shape[1], img_chopped_fin.shape[1]))\n",
    "  sorted_by_lower_bound = sorted(shelf_tuple, key=lambda tup: tup[0])\n",
    "  merged = []\n",
    "\n",
    "  for higher in sorted_by_lower_bound:\n",
    "                  if not merged:\n",
    "                      merged.append(higher)\n",
    "                  else:\n",
    "                      lower = merged[-1]\n",
    "                      if higher[0] <= lower[1]:\n",
    "                          upper_bound = max(lower[1], higher[1])\n",
    "                          merged[-1] = (lower[0], upper_bound)  # replace by merged interval\n",
    "                      else:\n",
    "                          merged.append(higher)\n",
    "\n",
    "  remove = merged\n",
    "  img_chopped_completed = img_chopped_fin.copy()\n",
    "### zatim zakomentavane, TD: zjistit, o kolik se tu orizne -> pricist k left_chop\n",
    "#  down_grade = 0\n",
    "#  for i in remove:\n",
    "#              #img_chopped_fin = img_chopped_fin[:,0:i[0] - down_grade + 1,:] + img_chopped_fin[:,i[1]- down_grade - 1:,:]\n",
    "#              img_chopped_completed = img_chopped_completed[:, np.r_[0:i[0] - down_grade + 1,i[1]- down_grade - 1: img_chopped_completed.shape[1]], :]\n",
    "#              down_grade += i[1] - i[0]\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "### extimate real measures\n",
    "\n",
    "  can_rb_x = 7\n",
    "  can_rb_y = 15\n",
    "\n",
    "  true_sizes_x =  img_chopped_completed.shape[1]/np.median(sizes_x_arr)*can_rb_x\n",
    "  true_sizes_y =  img_chopped_completed.shape[0]/np.median(sizes_y_arr)*can_rb_y\n",
    "\n",
    "# memory\n",
    "  #K.clear_session()\n",
    "  #tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "######### recognition automl model ############\n",
    "  \n",
    "  writelog('AutoML calling...')\n",
    "\n",
    "  #prediction_client = automl_v1beta1.PredictionServiceClient()\n",
    "  #project_id = 'saleshouse-prototype'\n",
    "  #model_id = 'ICN4760874677604180454' \n",
    "  #model_id = 'ICN7606729482481501366'\n",
    "  #model_id = 'ICN8555118107281638010'\n",
    "  #name = 'projects/{}/locations/us-central1/models/{}'.format(project_id, model_id)\n",
    "  ###################################################\n",
    "\n",
    "  ######### load local recognition model ###############\n",
    "\n",
    "  # Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
    "  #\n",
    "  # Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  # you may not use this file except in compliance with the License.\n",
    "  # You may obtain a copy of the License at\n",
    "  #\n",
    "  #     http://www.apache.org/licenses/LICENSE-2.0\n",
    "  \n",
    "  #writelog('1.')\n",
    "  model_file = \"{}output_graph.pb\".format(modelpath)\n",
    "  label_file = \"{}output_labels.txt\".format(modelpath)\n",
    "  #writelog('1.')\n",
    "  graph_1 = load_graph(model_file)  \n",
    "  \n",
    "  #########################################################\n",
    "  #writelog('1.')\n",
    "  tmp = [(0,0) for i in range(len(detections_wo))]\n",
    "  tmp_2 = [(0,0) for i in range(len(detections_wo))]\n",
    "  tmp_3 = [(0,0) for i in range(len(detections_wo))]\n",
    "  tmp_4 = [(0,0) for i in range(len(detections_wo))]\n",
    "  tmp_5 = [(0,0) for i in range(len(detections_wo))]\n",
    "  tmp_6 = [(0,0) for i in range(len(detections_wo))]\n",
    "  tmp_7 = [(0,0) for i in range(len(detections_wo))]\n",
    "  \n",
    "  class_tuples = {'rb': tmp, 'rbsf':tmp_4, 'bigshock': tmp_2, 'rbblue':tmp_3, 'rbred': tmp_5, 'rbturned': tmp_6, 'unknown': tmp_7 }\n",
    "  class_names = class_tuples.keys()\n",
    "  no_same = 0\n",
    "  no_diff = 0\n",
    "  no = 0\n",
    "\n",
    "  \n",
    "\n",
    "  for e in range(len(extractions_wo)):\n",
    "   \n",
    "    #if len(extractions_wo[e]) > 0: \n",
    "    if extractions_wo[e].size != 0: \n",
    "      try:\n",
    "        #writelog('1.')\n",
    "        pom = cv2.cvtColor(extractions_wo[e], cv2.COLOR_RGB2BGR) \n",
    "        #writelog('1.')\n",
    "        cv2.imwrite('{}img1.jpg'.format(outputpath), pom)\n",
    "        print('create file OK')\n",
    "    \n",
    "        #with open('{}img1.jpg'.format(outputpath), 'rb') as ff:\n",
    "        #  content = ff.read()\n",
    "        #payload = {'image': {'image_bytes': content }}\n",
    "        #params = {}\n",
    "        #try:\n",
    "          #request = prediction_client.predict(name, payload, params)\n",
    "          #detections_wo[e]['class'] = request.payload[0].display_name\n",
    "        lbl, pst = local_recognizer('{}img1.jpg'.format(outputpath), graph_1)\n",
    "        detections_wo[e]['class'] = lbl \n",
    "        if detections_wo[e]['class'] in class_names:\n",
    "\n",
    "          #print(lbl)\n",
    "          #print(request.payload[0].display_name)\n",
    "          #print('---')\n",
    "\n",
    "          class_tuples[detections_wo[e]['class']][e] = (detections_wo[e]['box_points'][0],detections_wo[e]['box_points'][2])\n",
    "      except Exception as err:\n",
    "        print(\"chyba v predikci\", e)\n",
    "        writelog(err)\n",
    "        writelog('Iteration n.{} failed.'.format(e))\n",
    "  \n",
    "  print('automl OK')\n",
    "  writelog('AutoML recognition completed.')\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "### backlog\n",
    "  \n",
    "  #cv2.imwrite(\"./test_write.jpg\", img_chopped_completed)\n",
    "  #!gsutil cp \"./test_write.jpg\" \"gs://saleshousephotos/test.jpg\"\n",
    "  #no_diff\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "### color the shelves based on recognition\n",
    "\n",
    "  writelog('Data preparation and DB load running.')\n",
    "  \n",
    "  tmp = bucket.blob(photos).download_as_string()\n",
    "  img3 = Img.open(io.BytesIO(tmp))\n",
    "  img3 = np.array(img3)\n",
    "  \n",
    "  if which_rot != 0:\n",
    "      img3 = rotateImage(img3, which_rot)\n",
    "  \n",
    "  #pred_colored = colour_shelves_prediction(detections_wo, img)\n",
    "  detections_wo_shift = shiftDetections(copy.deepcopy(detections_wo), left_chop)\n",
    "  \n",
    "  pred_colored = colour_shelves_prediction(detections_wo_shift, img3)\n",
    "  predicted_photo = cv2.cvtColor(pred_colored, cv2.COLOR_RGB2BGR)\n",
    "  cv2.imwrite(\"{}recognised_photo.jpg\".format(outputpath), predicted_photo)\n",
    "  bucket_name = output_bucket_name\n",
    "  bucket = storage_client.get_bucket(bucket_name)\n",
    "  blob = bucket.blob('recognised/' + jsn['name'])\n",
    "  blob.upload_from_filename('{}recognised_photo.jpg'.format(outputpath), content_type = 'image/jpg')\n",
    "  blob.make_public()\n",
    "\n",
    "  det_colored = colour_shelves(detections_wo_shift, img)\n",
    "  detected_photo = cv2.cvtColor(det_colored, cv2.COLOR_RGB2BGR)\n",
    "  cv2.imwrite(\"{}detected_photo.jpg\".format(outputpath), detected_photo)\n",
    "  bucket_name = output_bucket_name\n",
    "  bucket = storage_client.get_bucket(bucket_name)\n",
    "  blob = bucket.blob('detected/' + jsn['name'])\n",
    "  blob.upload_from_filename('{}detected_photo.jpg'.format(outputpath), content_type = 'image/jpg')\n",
    "  blob.make_public()\n",
    "\n",
    "  pth = \"https://storage.googleapis.com/\" + bucket_name + \"/recognised/\" + jsn['name']\n",
    "  pth_raw = \"https://storage.googleapis.com/\" + jsn[\"bucket\"] + \"/\" + jsn['name']\n",
    "  pth_det = \"https://storage.googleapis.com/\" + bucket_name + \"/detected/\" + jsn['name']\n",
    "  #]Img.fromarray(pred_colored, 'RGB')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "### calculate shelf ratio (percentage of shelf covered)\n",
    "### and prepare to check planograms\n",
    "  \n",
    "  sz = img_chopped_completed.shape[1]\n",
    "  shelves_no = max(shlvs)\n",
    "  df = list()\n",
    "  objects= list()  ## for planograms\n",
    "      \n",
    "  for k in range(shelves_no+1):\n",
    "    this_shelf = list()\n",
    "    shelf_labels = list()    ### labels present in shelf\n",
    "    shelf_positions = list()  ### positions of cans in this shelf (both for checking planograms)\n",
    "    for clss in class_names:\n",
    "      cntr = 0\n",
    "      shelf_tuple = list()\n",
    "      for idx, l in enumerate(detections_wo):\n",
    "        if l['shelf'] == k:\n",
    "          shelf_tuple.append(class_tuples[clss][idx])\n",
    "          #if l['class'] == clss:\n",
    "          cntr = cntr + 1\n",
    "\n",
    "        \n",
    "      sorted_by_lower_bound = sorted(shelf_tuple, key=lambda tup: tup[0])\n",
    "      \n",
    "      merged = []\n",
    "\n",
    "      for higher in sorted_by_lower_bound:\n",
    "        if not merged:\n",
    "            merged.append(higher)\n",
    "        else:\n",
    "            lower = merged[-1]\n",
    "            if higher[0] <= lower[1]:\n",
    "                upper_bound = max(lower[1], higher[1])\n",
    "                merged[-1] = (lower[0], upper_bound)  # replace by merged interval\n",
    "            else:\n",
    "                merged.append(higher)\n",
    "    \n",
    "      if clss != 'rbturned':    ## rbturned will be an exception (for planograms rbturned==redbull)\n",
    "          shelf_labels.append(clss)\n",
    "          shelf_positions.append(merged)\n",
    "      else:\n",
    "          rbturned = merged\n",
    "          \n",
    "        \n",
    "      this_shelf.append({'label': clss, 'cans': merged})\n",
    "      \n",
    "      zastoupeni = sum([i[1] - i[0] for i in merged])/sz\n",
    "      plechovek = sum([i[1] - i[0] for i in merged])/np.median(sizes_x_arr)\n",
    "      df.append({ 'shelf': k, 'class': clss, 'count_cans' : np.ceil(plechovek), 'count_boxes' : cntr,  'representation' : zastoupeni})\n",
    "      #df_plan.append({ 'shelf': k, 'label': clss, 'cans':merged})\n",
    "      \n",
    "      #df.append({ 'shelf': k, 'class': clss, 'count_cans' : np.ceil(plechovek), 'representation' : zastoupeni})\n",
    "      #print(clss, k, zastoupeni, np.ceil(plechovek))\n",
    "    \n",
    "    objects.append({'label':shelf_labels, 'location':shelf_positions})  ### first shelf will be on objects[0], second on 1 etc.\n",
    "  \n",
    "  \n",
    "  for i in range(len(objects[k]['label'])):  ### find redbull index  \n",
    "      if objects[k]['label'][i] != 'rb': \n",
    "          ind_rb= i\n",
    "          rb_all = objects[k]['location'][i]\n",
    "  \n",
    "  for i in range(len(rbturned)):   ## binb rb_all and rbturned\n",
    "    rb_all.append(rbturned[i])\n",
    "    \n",
    "  sorted_by_lower_bound = sorted(rb_all, key=lambda tup: tup[0])   ### and sort and choose distinct\n",
    "  \n",
    "  merged = []\n",
    "\n",
    "  for higher in sorted_by_lower_bound:\n",
    "        if not merged:\n",
    "            merged.append(higher)\n",
    "        else:\n",
    "            lower = merged[-1]\n",
    "            if higher[0] <= lower[1]:\n",
    "                upper_bound = max(lower[1], higher[1])\n",
    "                merged[-1] = (lower[0], upper_bound)  # replace by merged interval\n",
    "            else:\n",
    "                merged.append(higher)\n",
    "      \n",
    "  objects[k]['location'][ind_rb] = merged   ### write it ro rb --> rbturned and rb together  \n",
    "     \n",
    "  \n",
    "  #get store, user from photo metadata\n",
    "  user = jsn['metadata']['Uploader-Name']\n",
    "  store = jsn['metadata']['Store-Address']\n",
    "  retailer = store.split(\" \")[0]\n",
    "  address = \" \".join(store.split(\" \")[1:])\n",
    "    \n",
    "## check planogramu\n",
    "  products = [\"\", \"Red Bull\", \"Shock!\", \"Red Bull Red\", \"Red Bull Blue\", \"Red Bull Sugar Free\", \"Red Bull Bez Cukru\"]\n",
    "  labels = [\"unknown\", \"rb\", \"bigshock\", \"rbred\", \"rbblue\", \"rbsf\", \"rbsf\"]\n",
    "\n",
    "  plan_path = \"./parsed_planograms.pickle\"\n",
    "  with open(plan_path, 'rb') as handle:\n",
    "    planograms = pickle.load(handle)\n",
    "   \n",
    "  \n",
    "  col = 1  ## for now \n",
    "  \n",
    "  ### just to be present\n",
    "  for i  in range(len(objects)):\n",
    "      store_loc = parse_store_location(store)\n",
    "      plan_check_df = planogram_check(objects[i], planograms, row= i, column= col, store_location=store_loc, date=\"2018-11\")\n",
    "      \n",
    "    \n",
    "  \n",
    " \n",
    "  output_DF = pd.DataFrame(df)    \n",
    "\n",
    "  output_DF['url_photo'] = pth_raw\n",
    "  output_DF['url_photo_detected'] = pth_det\n",
    "  output_DF['url_photo_recognised'] = pth\n",
    "  output_DF['id_photo'] = int(jsn['generation'])\n",
    "  output_DF['unique_id'] = output_DF[\"id_photo\"].map(str) + output_DF[\"class\"] + output_DF['shelf'].map(str)\n",
    "  #output_DF['unique_id'] = output_DF[\"id_photo\"].map(str)  + output_DF['shelf'].map(str)\n",
    "  output_DF['shelf_sizes_est_x'] = int(true_sizes_x)\n",
    "  output_DF['shelf_sizes_est_y'] = 15\n",
    "  output_DF['representation'] = round(output_DF['representation'], 2)\n",
    "  output_DF['retailer'] = retailer\n",
    "  output_DF['store'] = address\n",
    "  output_DF['user'] = user\n",
    "  output_DF['shelf_column'] = 0\n",
    "  output_DF['planogram_eq_ratio'] = 'init'\n",
    "  output_DF['planogram_eq_ratio_extra'] = 'init'\n",
    "  output_DF['planogram_share'] = 0\n",
    "  output_DF['after_completation'] = 0\n",
    "  output_DF['photo_captured'] = photo_load\n",
    "  output_DF['photo_load'] = photo_load\n",
    "  output_DF['recognition_start'] = recognition_start\n",
    "  output_DF['load_type'] = load_type\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#Img.fromarray(img_chopped_fin, 'RGB')\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "### write to bigquery\n",
    "  \n",
    "  print('writing to BQ')\n",
    "\n",
    "  client = bq.Client()\n",
    "  dataset_id = bq_dataset\n",
    "  table_id = 'recognition_output'\n",
    "\n",
    "  def uploaddata(dataset_id, table_id, rows):\n",
    "      table_ref = client.dataset(dataset_id).table(table_id)\n",
    "      table = client.get_table(table_ref)\n",
    "      errors = client.insert_rows(table, rows)\n",
    "\n",
    "      assert errors == []\n",
    "\n",
    "  output_DF['count_boxes'] = output_DF['count_boxes'].map(int)\n",
    "  output_DF['db_insert'] = datetime.datetime.now(timezone('Europe/Berlin')).strftime    (\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    " \n",
    "\n",
    "  #url_photo_detected\n",
    "  sorted_DF = output_DF[['retailer', 'store', 'user', 'unique_id', 'id_photo', 'url_photo', 'url_photo_recognised',\n",
    "                         'url_photo_detected', 'shelf', 'class', 'representation', 'count_boxes', 'count_cans',\n",
    "                         'shelf_sizes_est_x', 'shelf_sizes_est_y', 'shelf_column', 'planogram_eq_ratio',\n",
    "                         'planogram_eq_ratio_extra', 'planogram_share', 'after_completation', 'photo_captured', \n",
    "                         'photo_load', 'recognition_start', 'db_insert', 'load_type']]\n",
    "\n",
    "  \n",
    "  tuples = [tuple(x) for x in sorted_DF.values]   \n",
    "  uploaddata(dataset_id, table_id, tuples)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#from datetime import datetime\n",
    "  #datetime_object = datetime.strptime('2018-11-21T14:03:42.757Z', '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "  \n",
    "  del(main_obj)  \n",
    "  del(detections)\n",
    "  del(detections_all)\n",
    "  del(extracted_obj)\n",
    "  del(extracted_all)\n",
    "  \n",
    "  gc.collect()\n",
    "  print('OK')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
