{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-automl\n",
    "!pip install google-cloud-storage\n",
    "!pip install opencv-python==3.3.0.9\n",
    "!pip install keras\n",
    "!pip install https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from google.datalab import storage\n",
    "from google.cloud import storage\n",
    "from IPython.display import Image\n",
    "from PIL import Image as Img\n",
    "from imageai.Detection import ObjectDetection\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "import io\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiates a client\n",
    "storage_client = storage.Client()\n",
    "\n",
    "# The name for the new bucket\n",
    "bucket_name = 'saleshousephotos'\n",
    "\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "blobs = bucket.list_blobs()\n",
    "\n",
    "photos = []\n",
    "for blob in blobs:\n",
    "  if 'photos' in blob.name:\n",
    "    photos.append(blob.name)\n",
    "photos = photos[1:]\n",
    "\n",
    "def blur_image(image, amount=5):\n",
    "    '''Blurs the image\n",
    "    Does not affect the original image'''\n",
    "    kernel = np.ones((amount, amount), np.float32) / (amount**2)\n",
    "    return cv2.filter2D(image, -1, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(photos)):\n",
    "for i in range(1):\n",
    "  tmp = bucket.blob(photos[i]).download_as_string()\n",
    "  img = Img.open(io.BytesIO(tmp))\n",
    "  img = np.array(img)\n",
    "    \n",
    "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "  edges = cv2.Canny(gray, 250, 250)\n",
    "  image = blur_image(edges, amount = 7)\n",
    "\n",
    "  lines = cv2.HoughLinesP(image, rho = 1, theta = math.pi, threshold = 20, minLineLength = img.shape[0]/1.5, maxLineGap = 0)\n",
    "\n",
    "  if lines is None:\n",
    "      lines = cv2.HoughLinesP(image, rho = 1, theta = math.pi, threshold = 50, minLineLength = img.shape[0]/2.5, maxLineGap = 3)\n",
    "\n",
    "  img2 = img.copy()\n",
    "  lines = sorted(lines,key=lambda x: x[0,0])\n",
    "  dominance = -1000\n",
    "  right_chop = img.shape[1]\n",
    "  left_chop = 0\n",
    "\n",
    "  for line2 in range(len(lines)):\n",
    "      line = lines[line2]\n",
    "      if line[0,0] - dominance > 0.2*img.shape[1]:\n",
    "          if (line[0,0] < 0.2*img.shape[1] or line[0,0] > 0.8*img.shape[1]):\n",
    "              if (line[0,0] < 0.2*img.shape[1]):\n",
    "                left_chop = line[0,0]\n",
    "              if (line[0,0] > 0.8*img.shape[1]):\n",
    "                right_chop = line[0,0]\n",
    "              pt1 = (line[0,0],line[0,1])\n",
    "              pt2 = (line[0,2],line[0,3])\n",
    "              cv2.line(img, pt1, pt2, (0,0,255), 3)\n",
    "              dominance = line[0,0]\n",
    "      pt1 = (line[0,0],line[0,1])\n",
    "      pt2 = (line[0,2],line[0,3])\n",
    "      cv2.line(img2, pt1, pt2, (0,0,255), 3)\n",
    "      \n",
    "  img_chopped = img[:,left_chop:right_chop,:]\n",
    "  PERC = 30\n",
    "\n",
    "  detector = ObjectDetection()\n",
    "  detector.setModelTypeAsRetinaNet()\n",
    "  detector.setModelPath(\"./temp_model.h5\")\n",
    "  detector.loadModel()\n",
    "\n",
    "  custom_objects = detector.CustomObjects(person=False, car=False, bottle = True)\n",
    "  main_obj, detections, extracted_obj = detector.detectCustomObjectsFromImage(input_image= img_chopped, \n",
    "                                                      input_type=\"array\",\n",
    "                                                      output_type = 'array', \n",
    "                                                      output_image_path= \"./im.png\", \n",
    "                                                      custom_objects=custom_objects,\n",
    "                                                      extract_detected_objects=True,\n",
    "                                                      minimum_percentage_probability=PERC)\n",
    "\n",
    "  detections_all = detections.copy()\n",
    "  img_chopped_black = img_chopped.copy()\n",
    "  \n",
    "  for i in range(len(detections)):\n",
    "    img_chopped_black[detections[i]['box_points'][1]:detections[i]['box_points'][3],detections[i]['box_points'][0]:detections[i]['box_points'][2],:] = 0\n",
    "  PERC = 5\n",
    "\n",
    "  detector = ObjectDetection()\n",
    "  detector.setModelTypeAsRetinaNet()\n",
    "  detector.setModelPath(\"./temp_model.h5\")\n",
    "  detector.loadModel()\n",
    "\n",
    "  custom_objects = detector.CustomObjects(person=False, car=False, bottle = True)\n",
    "  main_obj, detections, extracted_obj = detector.detectCustomObjectsFromImage(input_image= img_chopped_black, \n",
    "                                                      input_type=\"array\",\n",
    "                                                      output_type = 'array', \n",
    "                                                      output_image_path= \"./im.png\", \n",
    "                                                      custom_objects=custom_objects,\n",
    "                                                      extract_detected_objects=True,\n",
    "                                                      minimum_percentage_probability=PERC)\n",
    "  for i in detections:        \n",
    "    detections_all.append(i)\n",
    "  \n",
    "  shelf_tuple = [(l['box_points'][0], l['box_points'][2]) for l in detections_all]\n",
    "  shelf_tuple.append((0,0))\n",
    "  shelf_tuple.append((img_chopped.shape[1], img_chopped.shape[1]))\n",
    "  sorted_by_lower_bound = sorted(shelf_tuple, key=lambda tup: tup[0])\n",
    "  merged = []\n",
    "\n",
    "  for higher in sorted_by_lower_bound:\n",
    "        if not merged:\n",
    "            merged.append(higher)\n",
    "        else:\n",
    "            lower = merged[-1]\n",
    "            if higher[0] <= lower[1]:\n",
    "                upper_bound = max(lower[1], higher[1])\n",
    "                merged[-1] = (lower[0], upper_bound)  # replace by merged interval\n",
    "            else:\n",
    "                merged.append(higher)\n",
    "  remove = []\n",
    "  for i in range(len(merged)-1):\n",
    "    if merged[i+1][0]-merged[i][1] > img_chopped.shape[1]*0.05:\n",
    "      remove.append((merged[i][1], merged[i+1][0]))\n",
    "      \n",
    "  img_chopped_fin = img_chopped.copy()\n",
    "  down_grade = 0\n",
    "  for i in remove:\n",
    "    #img_chopped_fin = img_chopped_fin[:,0:i[0] - down_grade + 1,:] + img_chopped_fin[:,i[1]- down_grade - 1:,:]\n",
    "    img_chopped_fin = img_chopped_fin[:, np.r_[0:i[0] - down_grade + 1,i[1]- down_grade - 1: img_chopped_fin.shape[1]], :]\n",
    "    down_grade += i[1] - i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colour_frame(img, frame, width=2, colour=[0, 0, 0]):\n",
    "    \"\"\"\"\"\"\n",
    "    x1, y1, x2, y2 = frame['box_points']\n",
    "    img[y1:y2, x1-width:x1+width] = colour\n",
    "    img[y1:y2, x2-width:x2+width] = colour\n",
    "    img[y1-width:y1+width, x1:x2] = colour\n",
    "    img[y2-width:y2+width, x1:x2] = colour\n",
    "    return img\n",
    "\n",
    "\n",
    "def colour_map(shelf):\n",
    "    MAP = {0: [255, 0, 0], 1: [0, 0, 255], 2: [0, 255, 0], 3: [0, 255, 255], 4: [255, 255, 0]}\n",
    "    if shelf < 0:\n",
    "        return [0, 0, 0 ]\n",
    "    return MAP.get(shelf % len(MAP))\n",
    "\n",
    "\n",
    "def compute_dominance_relations(detections):\n",
    "    \"\"\"\"\"\"\n",
    "    for i, frame in enumerate(detections):\n",
    "        x1, y1, x2, y2 = frame['box_points']\n",
    "        for j, frame_ in enumerate(detections):\n",
    "            x1_, y1_, x2_, y2_ = frame_['box_points']\n",
    "            if y2 < y1_:\n",
    "                dominated_by = frame_.setdefault('dominated_by', set())\n",
    "                dominated_by.add(i)\n",
    "\n",
    "\n",
    "def compute_seed_shelves(detections, polish=1/4):\n",
    "    \"\"\"\"\"\"\n",
    "    frame_indeces = {i for i in range(len(detections))}    \n",
    "    shelf = -1\n",
    "    shelves = []\n",
    "    while frame_indeces:\n",
    "        shelf_frames = set()  \n",
    "        for i in frame_indeces:\n",
    "            frame = detections[i]\n",
    "#            current_frame = colour_frame(main_obj, frame)\n",
    "#            Image.fromarray(current_frame, 'RGB').show()\n",
    "            dominated_by = (detections[i] for i in frame.get('dominated_by', {}))\n",
    "            if all(dominating_frame.get('shelf', float('inf')) <= shelf \n",
    "                   for dominating_frame in dominated_by):\n",
    "                x1, y1, x2, y2 = frame['box_points']\n",
    "                upper_shelf_bottoms = [detections[i]['box_points'][3] for i in shelves[-1]] if shelf > -1 else []\n",
    "                if sum(y2_ >= y2 - GAP for y2_ in upper_shelf_bottoms) > polish*len(upper_shelf_bottoms):\n",
    "#                if all(y2_ >= y2 - GAP for y2_ in upper_shelf_bottoms):\n",
    "\n",
    "                    frame['shelf'] = shelf\n",
    "                    shelves[-1].add(i)\n",
    "                else:\n",
    "                    frame['shelf'] = shelf + 1\n",
    "                    next_shelf = True\n",
    "                    shelf_frames.add(i)\n",
    "#                main_obj = colour_frame(main_obj, frame, colour=colour_map(shelf))\n",
    "#                img = Image.fromarray(main_obj, 'RGB')\n",
    "        frame_indeces -= shelf_frames\n",
    "        if shelf > -1:\n",
    "            frame_indeces -= shelves[-1]\n",
    "        shelves.append(shelf_frames)\n",
    "\n",
    "        shelf += 1 if next_shelf else 0\n",
    "    return detections\n",
    "\n",
    "\n",
    "def detect_nonshelves(detections, main_obj):\n",
    "    \"\"\"\"\"\"\n",
    "    shelves = [set()  for _ in range(20)]\n",
    "    for iframe, frame in enumerate(detections):\n",
    "        shelves[frame['shelf']].add(iframe)\n",
    "        \n",
    "    image_width = main_obj.shape[1]\n",
    "    to_remove = []\n",
    "    for ishelf, shelf in enumerate(shelves[:-1]):\n",
    "        lower_tops = [(detections[j]['box_points'][1], detections[j]['box_points'][2]) for j in shelves[ishelf+1]]\n",
    "        if not lower_tops:\n",
    "            continue\n",
    "        noshelf = []\n",
    "        for i in shelf:\n",
    "            frame = detections[i]\n",
    "            frame_bottom = frame['box_points'][3]\n",
    "            frame_top = frame['box_points'][1]\n",
    "            if sum(y1 < frame_bottom for y1, y2 in lower_tops) > 0*len(lower_tops)/2:\n",
    "#                frame['shelf'] = -1\n",
    "                noshelf.append(frame)\n",
    "#        Image.fromarray(colour_shelves(noshelf, main_obj.copy()), 'RGB').show()\n",
    "        safety_condition = noshelf and (min(f['box_points'][0] for f in noshelf) > image_width*1/2\n",
    "                        or max(f['box_points'][2] for f in noshelf) < image_width*1/2)\n",
    "        if safety_condition:\n",
    "            for f in noshelf:\n",
    "                f['shelf'] = -1\n",
    "            to_remove.extend(noshelf)\n",
    "    for frame in detections:\n",
    "        if to_remove:\n",
    "            if frame['box_points'][2] > image_width/2:\n",
    "                if frame['box_points'][2] >= min(f['box_points'][0] for f in to_remove) > image_width/2:\n",
    "                    frame['shelf'] = -1\n",
    "            elif frame['box_points'][2] < image_width/2:\n",
    "                if frame['box_points'][0] <= max(f['box_points'][2] for f in to_remove) < image_width/2:\n",
    "                    frame['shelf'] = -1\n",
    "    return detections\n",
    "\n",
    "\n",
    "def detect_shelves(detections, polish=1/4):\n",
    "    \"\"\"Detekuje regaly na zaklade detekci lahvi. Vadi FALSE POSITIVES.\n",
    "    V prvni iteraci je ale treba ponechat vysi FALSE POSITVES, abychom meli nizsi FALSE NEGATIVES,\n",
    "    nebot FALSE NEGATIVES zase budou vadit nasledujicimu `correct` kroku.\n",
    "    V pripade ze se v dalsich iteracich snizi FALSE POSITIVES, bude mozna treba zvysovat parametr `polish` smerem k 0.99.\n",
    "    Parametr polish sleva \"falesne regaly\" do jednoho.\n",
    "    \n",
    "    Kazdy dict v `detections` dostane novy atribut `shelf` \n",
    "    \"\"\"\n",
    "    compute_dominance_relations(detections)   \n",
    "    detections = compute_seed_shelves(detections, polish)\n",
    "    return detections\n",
    "\n",
    "def correct_shelves(detections, main_obj):\n",
    "    \"\"\"Odstrani lahve stojici v sousednich regalech. Vadi FALSE NEGATIVES. \n",
    "    \n",
    "    Prvky v `detections` k odstraneni dostanou `shelf=-1`\n",
    "    \n",
    "    Input\n",
    "    ----\n",
    "    detections : list of dicts\n",
    "        detections with `shelf` attribute from `detect` method\n",
    "    main_obj : array \n",
    "        image from the bottle detector\n",
    "    \"\"\"\n",
    "    detections = detect_nonshelves(detections, main_obj)\n",
    "    return detections\n",
    "        \n",
    "def colour_shelves(detections, main_obj):\n",
    "    \"\"\"\"\"\"\n",
    "    for frame in detections:\n",
    "        if 'shelf' in frame:\n",
    "            main_obj = colour_frame(main_obj, frame, colour=colour_map(frame['shelf']))\n",
    "    return main_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAP = 15\n",
    "detections = detect_shelves(detections_all) # zaradi do regalu\n",
    "detections = correct_shelves(detections, img_chopped_fin)  # oznaci ty co nejsou v zadnem regalu\n",
    "main_obj = colour_shelves(detections, img_chopped_fin.copy())\n",
    "#img_chopped_fin\n",
    "Img.fromarray(main_obj, 'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "        shelf_tuple = list()\n",
    "        for l in detections:\n",
    "            if l['shelf'] == -1:\n",
    "                shelf_tuple.append( (l['box_points'][0], l['box_points'][2]) )\n",
    "        shelf_tuple.append((0,0))\n",
    "        shelf_tuple.append((img_chopped_fin.shape[1], img_chopped_fin.shape[1]))\n",
    "        sorted_by_lower_bound = sorted(shelf_tuple, key=lambda tup: tup[0])\n",
    "        merged = []\n",
    "        \n",
    "        for higher in sorted_by_lower_bound:\n",
    "                if not merged:\n",
    "                    merged.append(higher)\n",
    "                else:\n",
    "                    lower = merged[-1]\n",
    "                    if higher[0] <= lower[1]:\n",
    "                        upper_bound = max(lower[1], higher[1])\n",
    "                        merged[-1] = (lower[0], upper_bound)  # replace by merged interval\n",
    "                    else:\n",
    "                        merged.append(higher)\n",
    "        #remove = []\n",
    "        #for i in range(len(merged)-1):\n",
    "        #    if merged[i+1][0]-merged[i][1] > img_chopped_fin.shape[1]*0.05:\n",
    "        #      remove.append((merged[i][1], merged[i+1][0]))\n",
    "        \n",
    "        remove = merged\n",
    "        img_chopped_completed = img_chopped_fin.copy()\n",
    "        down_grade = 0\n",
    "        for i in remove:\n",
    "            #img_chopped_fin = img_chopped_fin[:,0:i[0] - down_grade + 1,:] + img_chopped_fin[:,i[1]- down_grade - 1:,:]\n",
    "            img_chopped_completed = img_chopped_completed[:, np.r_[0:i[0] - down_grade + 1,i[1]- down_grade - 1: img_chopped_completed.shape[1]], :]\n",
    "            down_grade += i[1] - i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Img.fromarray(img_chopped_completed, 'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"./test_write.jpg\", img_chopped_completed)\n",
    "!gsutil cp \"./test_write.jpg\" \"gs://saleshousephotos/test.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_client = automl_v1beta1.PredictionServiceClient()\n",
    "project_id = 'saleshouse-prototype'\n",
    "model_id = 'ICN4760874677604180454'\n",
    "name = 'projects/{}/locations/us-central1/models/{}'.format(project_id, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [(0,0) for i in range(len(detections))]\n",
    "class_tuples = {'RB': tmp, 'RBsf':tmp}\n",
    "class_names = class_tuples.keys()\n",
    "for e in range(len(extracted_obj)):\n",
    "  cv2.imwrite(\"./img1.jpg\", extracted_obj[e])\n",
    "  with open(\"./img1.jpg\", 'rb') as ff:\n",
    "    content = ff.read()\n",
    "  payload = {'image': {'image_bytes': content }}\n",
    "  params = {}\n",
    "  request = prediction_client.predict(name, payload, params)\n",
    "  detections[e]['class'] = request.payload[0].display_name\n",
    "  if detections[e]['class'] in class_names:\n",
    "    class_tuples[detections[e]['class']][e] = (detections[e]['box_points'][0],detections[e]['box_points'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_obj = colour_shelves_prediction(shelves, detections, main_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = main_obj.shape[1]\n",
    "for clss in class_names:\n",
    "  for k in range(len(shelves)):\n",
    "    shelf_tuple = [class_tuples[clss][l] for l in shelves[k]]\n",
    "    sorted_by_lower_bound = sorted(shelf_tuple, key=lambda tup: tup[0])\n",
    "    merged = []\n",
    "\n",
    "    for higher in sorted_by_lower_bound:\n",
    "      if not merged:\n",
    "          merged.append(higher)\n",
    "      else:\n",
    "          lower = merged[-1]\n",
    "          if higher[0] <= lower[1]:\n",
    "              upper_bound = max(lower[1], higher[1])\n",
    "              merged[-1] = (lower[0], upper_bound)  # replace by merged interval\n",
    "          else:\n",
    "              merged.append(higher)\n",
    "    zastoupeni = sum([i[1] - i[0] for i in merged])/sz \n",
    "    print(clss, k, zastoupeni)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
